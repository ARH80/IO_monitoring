Index: src/tensorflow_apps/mnist-classification-with-keras.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import numpy as np\n\nnp.random.seed(123)\n\nfrom matplotlib import pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D\n\nfrom keras.utils import np_utils\nfrom keras.datasets import mnist\n\n(X_train, y_train), (X_test, y_test) = mnist.load_data()\nplt.imshow(X_train[0])\nplt.show()\n\nX_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\nX_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n\nX_train = X_train.astype('float32')\nX_test = X_test.astype('float32')\n\nX_train /= 255\nX_test /= 255\n\nY_train = np_utils.to_categorical(y_train, 10)\nY_test = np_utils.to_categorical(y_test, 10)\n\nmodel = Sequential()\n\nmodel.add(Convolution2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\nmodel.add(Convolution2D(32, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nhist = model.fit(X_train, Y_train, batch_size=32, nb_epoch=1, verbose=1)\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/tensorflow_apps/mnist-classification-with-keras.py b/src/tensorflow_apps/mnist-classification-with-keras.py
--- a/src/tensorflow_apps/mnist-classification-with-keras.py	(revision 80c46d960e2312993d1289d8dc81c414bfc75197)
+++ b/src/tensorflow_apps/mnist-classification-with-keras.py	(date 1675515086608)
@@ -1,13 +1,12 @@
 import numpy as np
-
-np.random.seed(123)
-
 from matplotlib import pyplot as plt
-from keras.models import Sequential
-from keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D
 
-from keras.utils import np_utils
-from keras.datasets import mnist
+from tensorflow.keras.models import Sequential
+from tensorflow.keras.datasets import mnist
+from tensorflow.keras.utils import to_categorical
+from tensorflow.keras.layers import Dense, Dropout, Flatten, Convolution2D, MaxPooling2D
+
+np.random.seed(123)
 
 (X_train, y_train), (X_test, y_test) = mnist.load_data()
 plt.imshow(X_train[0])
@@ -22,8 +21,8 @@
 X_train /= 255
 X_test /= 255
 
-Y_train = np_utils.to_categorical(y_train, 10)
-Y_test = np_utils.to_categorical(y_test, 10)
+Y_train = to_categorical(y_train, 10)
+Y_test = to_categorical(y_test, 10)
 
 model = Sequential()
 
Index: src/tensorflow_apps/cifar-100-classification-with-keras.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from tensorflow.keras.datasets import cifar100\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\n\nimport matplotlib.pyplot as plt\n\n# Model configuration\nbatch_size = 50\nimg_width, img_height, img_num_channels = 32, 32, 3\nloss_function = sparse_categorical_crossentropy\nno_classes = 100\nno_epochs = 1\noptimizer = Adam()\nvalidation_split = 0.2\nverbosity = 1\n\n# Load CIFAR-100 data\n(input_train, target_train), (input_test, target_test) = cifar100.load_data()\n\n# Determine shape of the data\ninput_shape = (img_width, img_height, img_num_channels)\n\n# Parse numbers as floats\ninput_train = input_train.astype('float32')\ninput_test = input_test.astype('float32')\n\n# Normalize data\ninput_train = input_train / 255\ninput_test = input_test / 255\n\n# Create the model\nmodel = Sequential()\nmodel.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dense(no_classes, activation='softmax'))\n\nmodel.compile(\n    loss=loss_function, optimizer=optimizer, metrics=['accuracy'])\n\nhistory = model.fit(\n    input_train, target_train,\n    batch_size=batch_size,\n    epochs=no_epochs,\n    verbose=verbosity,\n    validation_split=validation_split)\n\n# Generate generalization metrics\nscore = model.evaluate(input_test, target_test, verbose=0)\nprint(f'Test loss: {score[0]} / Test accuracy: {score[1]}')\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/tensorflow_apps/cifar-100-classification-with-keras.py b/src/tensorflow_apps/cifar-100-classification-with-keras.py
--- a/src/tensorflow_apps/cifar-100-classification-with-keras.py	(revision 80c46d960e2312993d1289d8dc81c414bfc75197)
+++ b/src/tensorflow_apps/cifar-100-classification-with-keras.py	(date 1675514653091)
@@ -4,33 +4,24 @@
 from tensorflow.keras.losses import sparse_categorical_crossentropy
 from tensorflow.keras.optimizers import Adam
 
-import matplotlib.pyplot as plt
-
-# Model configuration
-batch_size = 50
+batch_size = 512
 img_width, img_height, img_num_channels = 32, 32, 3
-loss_function = sparse_categorical_crossentropy
 no_classes = 100
-no_epochs = 1
+no_epochs = 10
+
+loss_function = sparse_categorical_crossentropy
 optimizer = Adam()
-validation_split = 0.2
-verbosity = 1
 
-# Load CIFAR-100 data
 (input_train, target_train), (input_test, target_test) = cifar100.load_data()
 
-# Determine shape of the data
 input_shape = (img_width, img_height, img_num_channels)
 
-# Parse numbers as floats
 input_train = input_train.astype('float32')
 input_test = input_test.astype('float32')
 
-# Normalize data
 input_train = input_train / 255
 input_test = input_test / 255
 
-# Create the model
 model = Sequential()
 model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))
 model.add(MaxPooling2D(pool_size=(2, 2)))
@@ -50,9 +41,8 @@
     input_train, target_train,
     batch_size=batch_size,
     epochs=no_epochs,
-    verbose=verbosity,
-    validation_split=validation_split)
+    verbose=1,
+    validation_split=0.2)
 
-# Generate generalization metrics
 score = model.evaluate(input_test, target_test, verbose=0)
 print(f'Test loss: {score[0]} / Test accuracy: {score[1]}')
Index: scripts/screens-monitoring.sh
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>#sudo sh scripts/screens-monitoring.sh \"/home/alireza/PycharmProjects/IO_monitoring/venv/bin/python\"\n#    \"src/tensorflow_apps\" \"cifar-100-classification-with-keras.py\" \"nvme0n1\" \"keras-classification\"\n\n\necho none > /sys/block/nvme0n1/queue/scheduler\nsync\necho 3 > /proc/sys/vm/drop_caches\n\nmkdir -p $2/$5\n\nscreen -dmS monitoring-screen-iostat bash -c \"time iostat -tx /dev/$4 1 > $2/$5/iostat.txt\"\nscreen -dmS monitoring-screen-blktrace bash -c \"time blktrace -d /dev/$4 -a complete -o - > $2/$5/trace.txt\"\n\n$1 $2/$3\n\nscreen -XS monitoring-screen-iostat quit\nscreen -XS monitoring-screen-blktrace quit\n\ntime cat $2/$5/trace.txt | blkparse -i - > $2/$5/parsed_trace.txt\n\n$1 src/iostat_monitoring/iostat/main.py --data $2/$5/iostat.txt --disk $4 --output $2/$5/iostat.csv csv\n$1 src/iostat_monitoring/iostat/main.py --data $2/$5/iostat.txt --disk $4 --fig-output $2/$5/iostat-plot.png plot\n$1 src/blktrace_monitoring/blktrace_plot.py $2/$5\n\nchmod -R 777 $2/$5/iostat_cpu.csv\nchmod -R 777 $2/$5/iostat_devices.csv\nchmod -R 777 $2/$5\nchmod -R 777 $2/$5/trace.txt\nchmod -R 777 $2/$5/iostat.txt\nchmod -R 777 $2/$5/parsed_trace.txt\n\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/scripts/screens-monitoring.sh b/scripts/screens-monitoring.sh
--- a/scripts/screens-monitoring.sh	(revision 80c46d960e2312993d1289d8dc81c414bfc75197)
+++ b/scripts/screens-monitoring.sh	(date 1675509474918)
@@ -1,5 +1,5 @@
 #sudo sh scripts/screens-monitoring.sh "/home/alireza/PycharmProjects/IO_monitoring/venv/bin/python"
-#    "src/tensorflow_apps" "cifar-100-classification-with-keras.py" "nvme0n1" "keras-classification"
+#    "src/tensorflow_apps" "cifar-100-classification-with-keras.py" "nvme0n1" "keras-classification-cifar-100"
 
 
 echo none > /sys/block/nvme0n1/queue/scheduler
@@ -18,9 +18,9 @@
 
 time cat $2/$5/trace.txt | blkparse -i - > $2/$5/parsed_trace.txt
 
+$1 src/blktrace_monitoring/blktrace_plot.py $2/$5
 $1 src/iostat_monitoring/iostat/main.py --data $2/$5/iostat.txt --disk $4 --output $2/$5/iostat.csv csv
 $1 src/iostat_monitoring/iostat/main.py --data $2/$5/iostat.txt --disk $4 --fig-output $2/$5/iostat-plot.png plot
-$1 src/blktrace_monitoring/blktrace_plot.py $2/$5
 
 chmod -R 777 $2/$5/iostat_cpu.csv
 chmod -R 777 $2/$5/iostat_devices.csv
Index: requirements.txt
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>matplotlib\nseaborn\npandas\nnumpy\nopencv-python\ntensorflow\nkeras\ncmake\ndlib\ntqdm\ntorch\neinops\ntorchvision\nscikit-learn\nremotezip\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/requirements.txt b/requirements.txt
--- a/requirements.txt	(revision 80c46d960e2312993d1289d8dc81c414bfc75197)
+++ b/requirements.txt	(date 1675509474930)
@@ -12,4 +12,5 @@
 einops
 torchvision
 scikit-learn
+tensorflow_datasets
 remotezip
Index: src/tensorflow_apps/augmentation_layers.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/tensorflow_apps/augmentation_layers.py b/src/tensorflow_apps/augmentation_layers.py
new file mode 100644
--- /dev/null	(date 1675514555774)
+++ b/src/tensorflow_apps/augmentation_layers.py	(date 1675514555774)
@@ -0,0 +1,13 @@
+import tensorflow as tf
+
+image_size = 256
+crop_size = image_size / 2
+
+augmentation = tf.keras.Sequential([
+    tf.keras.layers.Resizing(image_size, image_size),
+    tf.keras.layers.RandomRotation(0.2),
+    tf.keras.layers.Rescaling(1. / 255),
+    tf.keras.layers.RandomZoom(0.5),
+    tf.keras.layers.RandomFlip(),
+    tf.keras.layers.RandomCrop(crop_size, crop_size),
+])
Index: src/blktrace_monitoring/blktrace_plot.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\nimport sys\n\nfrom collections import Counter\n\n\nclass BLK:\n\n    def __init__(self, directory):\n        self.directory = directory\n        self.df = None\n\n        with open(F'{directory}/parsed_trace.txt', 'r') as f:\n            self.lines = f.readlines()\n\n        copy_list = []\n        for line in self.lines:\n            if line.startswith('CPU'):\n                break\n\n            copy_list.append(line)\n\n        self.lines = copy_list\n        last = self.lines[-1]\n        last = float(last.split()[3])\n\n        self.time_scope_size = int(last // 100)\n        self.time_scope_size = max(self.time_scope_size, 1)\n        self.address_scope_size = 25 * 1e6\n\n    def get_features_dataframe(self):\n        def parser(record):\n            tokens = record.split()\n            result = {\n                'timestamp': float(tokens[3]),\n                'time_scope': float(tokens[3]) // self.time_scope_size,\n                'cid': int(tokens[1]),\n                'sid': int(tokens[2]),\n                'pid': int(tokens[4]),\n                'action': tokens[5],\n                'rw': tokens[6],\n                'rw_spec': 'R' if 'R' in tokens[6] else 'W' if 'W' in tokens[6] else 'N',\n                'start_address': int(tokens[7])\n            }\n\n            result.update({'n_sectors': int(tokens[9]) if '+' in tokens else 1})\n            result.update({'size': result['n_sectors'] * 512})\n            result.update({'end_address': result['start_address'] + result['size']})\n\n            result.update({\n                'scope_start_address': int(result['start_address'] // self.address_scope_size),\n                'scope_end_address': int(result['start_address'] // self.address_scope_size)\n            })\n\n            return result\n\n        df = pd.DataFrame([parser(x) for x in self.lines])\n        self.df = df[df.rw_spec != 'N']\n\n    def pie_plot(self):\n        palette_color = sns.color_palette('bright')\n\n        _ = plt.figure()\n        plt.pie(\n            [self.df[self.df.rw_spec == 'R'].shape[0], self.df[self.df.rw_spec == 'W'].shape[0]],\n            labels=['Read', 'Write'],\n            colors=palette_color,\n            autopct='%.0f%%')\n\n        plt.title('Pie chart.')\n        plt.savefig(F'{self.directory}/pie')\n\n    def density_on_size(self):\n        _ = plt.figure()\n        self.df.plot(kind='hist', column=['size'], by='rw_spec', bins=50)\n        plt.savefig(F'{self.directory}/density_on_size.png')\n\n    def rw_intensive_plot(self):\n        def count_R(group):\n            return group[group == 'R'].shape[0]\n\n        def count_W(group):\n            return group[group == 'W'].shape[0]\n\n        df = self.df.groupby(['time_scope']).agg(\n            r_count=('rw_spec', count_R), w_count=('rw_spec', count_W))\n\n        df['WI'] = df[['r_count', 'w_count']].apply(\n            lambda row: row[0] < row[1], axis=1).astype(int)\n\n        df['RI'] = df[['r_count', 'w_count']].apply(\n            lambda row: row[1] < row[0], axis=1).astype(int)\n\n        _ = plt.figure()\n        df[['WI', 'RI']].plot(kind='bar')\n        plt.title('Read or Write intensive')\n        plt.savefig(F'{self.directory}/rw_intensive.png')\n\n    def scope_frequency(self):\n        c = Counter()\n        _ = self.df[['scope_start_address', 'scope_end_address']].apply(\n            lambda row: c.update(list(range(row[0], row[1] + 1))), axis=1)\n\n        scopes, counts = zip(*c.items())\n\n        _ = plt.figure()\n        plt.bar(scopes, counts)\n        plt.title('Address Freq.s')\n        plt.savefig(F'{self.directory}/address_frequency.png')\n\n    def hot_and_cold_scopes(self):\n\n        time_scopes = self.df.time_scope.tolist()\n        counters = {s: Counter() for s in time_scopes}\n\n        df = self.df.astype({\n            'scope_start_address': int, 'time_scope': int, 'scope_end_address': int,\n        })\n\n        start_address = df['scope_start_address'].min()\n        start_time = df['time_scope'].min()\n        end_time = df['time_scope'].max()\n        end_address = df['scope_end_address'].max()\n\n        counters = np.zeros((end_address - start_address + 1, end_time - start_time + 1))\n\n        def update(row):\n            for x in range(row[0], row[1] + 1):\n                counters[x - start_address, row[2] - start_time] += 1\n\n        _ = df[['scope_start_address', 'scope_end_address', 'time_scope']].apply(lambda row: update(row), axis=1)\n\n        _ = plt.figure()\n        sns.heatmap(np.log(counters + 1), cmap=\"crest\")\n        plt.title('Hot and Cold scopes')\n        plt.xlabel('time_scopes')\n        plt.ylabel('address_scope')\n        plt.savefig(F'{self.directory}/hot_and_cold_scopes.png')\n\n\nblk = BLK(sys.argv[1])\nblk.get_features_dataframe()\n\nblk.density_on_size()\nblk.pie_plot()\nblk.scope_frequency()\nblk.rw_intensive_plot()\nblk.hot_and_cold_scopes()\n\nplt.savefig('./blk-4main-plots.png')\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/blktrace_monitoring/blktrace_plot.py b/src/blktrace_monitoring/blktrace_plot.py
--- a/src/blktrace_monitoring/blktrace_plot.py	(revision 80c46d960e2312993d1289d8dc81c414bfc75197)
+++ b/src/blktrace_monitoring/blktrace_plot.py	(date 1675508003171)
@@ -107,10 +107,29 @@
 
         scopes, counts = zip(*c.items())
 
+        counts = np.array(counts)
+        counts = counts / counts.sum()
+
         _ = plt.figure()
         plt.bar(scopes, counts)
         plt.title('Address Freq.s')
-        plt.savefig(F'{self.directory}/address_frequency.png')
+        plt.savefig(F'{self.directory}/address_scope_frequency.png')
+
+    def scope_frequency_cdf(self):
+        c = Counter()
+        _ = self.df[['scope_start_address', 'scope_end_address']].apply(
+            lambda row: c.update(list(range(row[0], row[1] + 1))), axis=1)
+
+        scopes, counts = zip(*c.items())
+
+        counts = np.array(counts)
+        counts = counts / counts.sum()
+        counts = np.cumsum(counts)
+
+        _ = plt.figure()
+        plt.bar(scopes, counts)
+        plt.title('Address Freq.s')
+        plt.savefig(F'{self.directory}/address_scope_freq_cdf.png')
 
     def hot_and_cold_scopes(self):
 
@@ -150,5 +169,4 @@
 blk.scope_frequency()
 blk.rw_intensive_plot()
 blk.hot_and_cold_scopes()
-
-plt.savefig('./blk-4main-plots.png')
+blk.scope_frequency_cdf()
